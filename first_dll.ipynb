{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/erconui/cs501r/blob/master/first_dll.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "RLWT5ZpwgK4u",
    "outputId": "0a5465ca-4de4-4b4a-cc29-358e4e8e57dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: torchvision in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (from torchvision) (0.4.1)\n",
      "Requirement already satisfied: numpy in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (from torchvision) (1.15.1)\n",
      "Requirement already satisfied: six in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (from torchvision) (5.2.0)\n",
      "Requirement already satisfied: tqdm in /home/mouse/.virtualenvs/deep/lib/python3.6/site-packages (4.26.0)\n",
      "Collecting torchsummary\n",
      "  Downloading https://files.pythonhosted.org/packages/57/a8/f935291ecb02228ad2a114a55ceb32345d6d722d27a2861d230fcca11096/torchsummary-1.5-py3-none-any.whl\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch \n",
    "!pip3 install torchvision\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96fllQAiez3q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import pdb\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert torch.cuda.is_available() # You need to request a GPU from Runtime > Change Runtime Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqfzWKGVcHyU"
   },
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, \n",
    "               padding=0, dilation=1, groups=1, bias=True, initialization=0):\n",
    "    self.__dict__.update(locals())\n",
    "    super(Conv2d, self).__init__()\n",
    "    \n",
    "    self.weight = Parameter(torch.Tensor(out_channels, in_channels,\n",
    "                                         *kernel_size))\n",
    "    \n",
    "    self.bias = Parameter(torch.Tensor(out_channels))\n",
    "    \n",
    "    if initialization==1:\n",
    "        ## Uniform Initialization\n",
    "        self.weight.data.uniform_(-1,1)\n",
    "        self.bias.data.uniform_(0,0)\n",
    "    if initialization==2:\n",
    "        ## XE Initialization\n",
    "        self.weight.data.fill_(0.01)\n",
    "        self.bias.data.fill_(0.01)\n",
    "    if initialization==3:\n",
    "        ## Orthogonal Initialization\n",
    "        self.init_orthogonal(in_channels, out_channels, kernel_size)\n",
    "\n",
    "  def init_orthogonal(self, in_channels, out_channels, kernel_size):\n",
    "    width = in_channels*kernel_size[0]*kernel_size[1]\n",
    "    X = np.random.random((out_channels, width))\n",
    "    U, _, VT = np.linalg.svd(X,full_matrices=False)\n",
    "    if out_channels > width:\n",
    "      weight = U\n",
    "    else:\n",
    "      weight = VT\n",
    "    weight = weight.reshape((out_channels, in_channels, kernel_size[0], kernel_size[1]))\n",
    "    self.weight = Parameter(torch.tensor(weight).float())\n",
    "    \n",
    "  def forward(self, x):\n",
    "    return F.conv2d(x, self.weight, self.bias, self.stride, self.padding,\n",
    "                    self.dilation, self.groups)\n",
    "  \n",
    "  def extra_repr(self):\n",
    "    return '501r is so cool'\n",
    "\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "  def __init__(self, weight=None, size_average=None, ignore_index=-100,reduce=None, reduction='elementwise_mean'):\n",
    "    super(CrossEntropyLoss, self).__init__()\n",
    "    \n",
    "  def forward(self, x, labels):\n",
    "    alpha = torch.max(x).item()\n",
    "    softmax = -torch.log(torch.exp(x-alpha)/torch.exp(x-alpha).sum(1, keepdim=True))\n",
    "    r = torch.arange(softmax.size(0))\n",
    "    return softmax[r, labels].mean()\n",
    "  \n",
    "class ConvNetwork(nn.Module):\n",
    "  def __init__(self, dataset, million_parameters=False):\n",
    "    super(ConvNetwork, self).__init__()\n",
    "    x, y = dataset[0]\n",
    "    c,h,w = x.size()\n",
    "    output = 10\n",
    "    \n",
    "    if million_parameters:\n",
    "        self.net = nn.Sequential(\n",
    "              nn.Conv2d(c, 100, (3,3), padding=(1,1)),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(100, 100, (3,3), padding=(1,1)),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(100, 100, (3,3), padding=(1,1)),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(100, output, (28,28), padding=(0,0))\n",
    "        )\n",
    "    else:\n",
    "        self.net = nn.Sequential(\n",
    "              nn.Conv2d(c, 10, (3,3), padding=(1,1)),\n",
    "              nn.ReLU(),\n",
    "              nn.Conv2d(10, output, (28,28), padding=(0,0))\n",
    "        )\n",
    "   \n",
    "  def forward(self, x):\n",
    "    return self.net(x).squeeze(2).squeeze(2)\n",
    "\n",
    "class FashionMNISTProcessedDataset(Dataset):\n",
    "  def __init__(self, root, train=True):\n",
    "    self.data = datasets.FashionMNIST(\n",
    "        root, train=train, transform=transforms.ToTensor(), download=True)\n",
    "    \n",
    "  def __getitem__(self, i):\n",
    "    x, y = self.data[i]\n",
    "    return x, y\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            total_param += np.prod(param.size())\n",
    "    return total_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "loss:0.4600:  25%|██▍       | 351/1429 [00:47<00:55, 19.37it/s]"
     ]
    }
   ],
   "source": [
    "def train_data(model, objective, optimizer, train_loader, val_loader):\n",
    "    losses = []\n",
    "    validations = []\n",
    "    accuracies = []\n",
    "    val_accuracies = []\n",
    "    for epoch in range(1):\n",
    "      loop = tqdm(total=len(train_loader), position=0)\n",
    "      for batch, (x, y_truth) in enumerate(train_loader):\n",
    "        x, y_truth = x.cuda(async=True), y_truth.cuda(async=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(x)\n",
    "\n",
    "        loss = objective(y_hat,y_truth)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        losses.append(loss)\n",
    "        accuracy = (torch.softmax(y_hat, 1).argmax(1) == y_truth).float().mean()\n",
    "        accuracies.append(accuracy)\n",
    "        loop.set_description('loss:{:.4f}'.format(loss.item()))\n",
    "        loop.update(1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "          vals = []\n",
    "          tmp_accuracies = []\n",
    "          for x1,y in val_loader:\n",
    "            x1, y_truth1 = x1.cuda(async=True), y.cuda(async=True)\n",
    "            y_hat = model(x1)\n",
    "            tmp = objective(y_hat, y_truth1).item()\n",
    "            vals.append(tmp)\n",
    "            accuracy = (y_hat.argmax(1) == y_truth1).float().mean()\n",
    "            tmp_accuracies.append(accuracy)\n",
    "\n",
    "          val_accuracies.append((len(losses), np.mean(tmp_accuracies)))\n",
    "          validations.append((len(losses), np.mean(vals)))\n",
    "\n",
    "      loop.close()\n",
    "    return validations, losses, val_accuracies, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QTGCuMiDcRED",
    "outputId": "61259ae7-0b3e-4e91-b7b4-42a8b99515e1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:2.3070:   0%|          | 0/1429 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters is:  875110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss:0.4600:  24%|██▍       | 350/1429 [00:32<00:55, 19.37it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-41cdb2776c30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m validations, losses, val_accuracies, accuracies = train_data(model, objective, optimizer,\n\u001b[0;32m---> 14\u001b[0;31m                                                              train_loader, val_loader)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalidations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# fig, ax = plt.subplots(1,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-a8fadb395a8a>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, objective, optimizer, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mtmp_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_truth1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-fd43118eede7>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/local/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2480\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2428\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m             im = im._new(\n\u001b[0;32m-> 2430\u001b[0;31m                 \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2431\u001b[0m                 )\n\u001b[1;32m   2432\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/deep/local/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = FashionMNISTProcessedDataset('/tmp/fashionmnist', train=True)\n",
    "val_dataset = FashionMNISTProcessedDataset('/tmp/fashionmnist', train=False)\n",
    "\n",
    "model = ConvNetwork(train_dataset, million_parameters=True)\n",
    "print(\"Number of Parameters is: \", count_parameters(model))\n",
    "\n",
    "model.cuda()\n",
    "objective = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=42, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=42, pin_memory=True)\n",
    "\n",
    "validations, losses, val_accuracies, accuracies = train_data(model, objective, optimizer,\n",
    "                                                             train_loader, val_loader)\n",
    "a, b = zip(*validations)\n",
    "# fig, ax = plt.subplots(1,2)\n",
    "plt.title(\"Loss with 1,000,000 Parameters\")\n",
    "plt.plot(losses, label='train')\n",
    "plt.plot(a, b, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.title(\"Accuracy with Xavier implementation\")\n",
    "plt.plot(accuracies, label='training accuracy')\n",
    "a, b = zip(*val_accuracies)\n",
    "plt.plot(a, b, label=\"validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done with Xavier Implementation\n",
    "model = ConvNetwork(train_dataset, )\n",
    "print(\"Number of Parameters is: \", count_parameters(model))\n",
    "\n",
    "model.cuda()\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=42, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=42, pin_memory=True)\n",
    "\n",
    "validations, losses, val_accuracies, accuracies = train_data(model, objective, optimizer,\n",
    "                                                             train_loader, val_loader)\n",
    "a, b = zip(*validations)\n",
    "# fig, ax = plt.subplots(1,2)\n",
    "plt.title(\"Loss with 1,000,000 Parameters\")\n",
    "plt.plot(losses, label='train')\n",
    "plt.plot(a, b, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.title(\"Accuracy with Xavier implementation\")\n",
    "plt.plot(accuracies, label='training accuracy')\n",
    "a, b = zip(*val_accuracies)\n",
    "plt.plot(a, b, label=\"validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = FashionMNISTProcessedDataset('/tmp/fashionmnist', train=True)\n",
    "val_dataset = FashionMNISTProcessedDataset('/tmp/fashionmnist', train=False)\n",
    "\n",
    "model = ConvNetwork(train_dataset)\n",
    "print(\"Number of Parameters is: \", count_parameters(model))\n",
    "\n",
    "model.cuda()\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_loader = DataLoader(train_dataset, batch_size=42, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=42, pin_memory=True)\n",
    "\n",
    "validations, losses, val_accuracies, accuracies = train_data(model, objective, optimizer,\n",
    "                                                             train_loader, val_loader)\n",
    "\n",
    "a, b = zip(*validations)\n",
    "# fig, ax = plt.subplots(1,2)\n",
    "plt.title(\"Loss with 1,000,000 Parameters\")\n",
    "plt.plot(losses, label='train')\n",
    "plt.plot(a, b, label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.title(\"Accuracy with Xavier implementation\")\n",
    "plt.plot(accuracies, label='training accuracy')\n",
    "a, b = zip(*val_accuracies)\n",
    "plt.plot(a, b, label=\"validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuarzmKqmlwo"
   },
   "outputs": [],
   "source": [
    "## Cross Entropy loss\n",
    "# check it outputs the same as pytorch's crossentropyloss\n",
    "\n",
    "#necessary for the soft max\n",
    "a = torch.from_numpy(np.random.randn(3,4,1).astype(np.float32))\n",
    "b = torch.exp(a)\n",
    "z = a / b.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D__Cy79Wtew4"
   },
   "source": [
    "# Part 4\n",
    "## Using a Kernel size of 3×3 what should the settings of your 2d convolution be that results in the following mappings (first answer given to you)\n",
    "\n",
    " (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(3, 3), padding=(0, 0))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : (out_channels=22, kernel_size=(3,3), padding=(1,1))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=65, h=12, w=12) : (out_channels=65, kernel_size=(3,3), padding=(2,2))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=7, h=20, w=20) : (out_channels=7, kernel_size=(3,3), padding=(12,12))\n",
    "\n",
    "## Using a Kernel size of 5×5:\n",
    "\n",
    " (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(5, 5), padding=(1, 1))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : (out_channels=100, kernel_size=(5,5), padding=(2,2))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : (out_channels=23, kernel_size=(5,5), padding=(3,3))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : (out_channels=5, kernel_size=(5,5), padding=(18,18))\n",
    "\n",
    "## Using Kernel size of 5×3:\n",
    "\n",
    " (c=3, h=10, w=10) ⇒ (c=10, h=8, w=8) : (out_channels=10, kernel_size=(5,3), padding=(2,0))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=100, h=10, w=10) : (out_channels=100, kernel_size=(5,3), padding=(4,2))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=23, h=12, w=12) : (out_channels=23, kernel_size=(5,3), padding=(6,4))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=5, h=24, w=24) : (out_channels=5, kernel_size=(5,3), padding=(18,16))\n",
    "\n",
    "## Determine the kernel that requires the smallest padding size to make the following mappings possible:\n",
    "\n",
    " (c=3, h=10, w=10) ⇒ (c=10, h=9, w=7) : (out_channels=10, kernel_size=(4,4), padding=(1,0))\n",
    "\n",
    "(c=3, h=10, w=10) ⇒ (c=22, h=10, w=10) : (out_channels=22, kernel_size=(1,1), padding=(0,0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "first dll.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
